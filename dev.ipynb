{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import os \n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"bofenghuang/whisper-large-v2-cv11-german\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/Users/svenasmussen/Documents/Code/huggingFaceModels/whisper-large-v2-cv11-german\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# Assuming `model` is your trained model and `processor` is your processor\n",
    "save_directory = \"models/whisper-large-v2-cv11-german\"\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the processor/tokenizer\n",
    "processor.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation using pipe (only works for large models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Musik) Aber immer sind wir nicht ganz dicht. Alle nicht ganz dicht in der Kuroase. Und wie sind Sie heute da? Mir geht es gar nicht gut, Frau Dr. Pettmöller. Dieser Bahnstreik, ich wollte heute mal schön aufs Festland und dann ist plötzlich dieser Wesselsky wieder. Oh ja, wo kommt der nun wieder her, ne? Ja, dat is Wesselsky. Jahrelang sieht man ihn nicht, spürt man ihn nicht, denn plötzlich ploppt er wieder auf ans Tageslicht. Dat is wie Howard Carbondale. So ähnlich. Dat Phänomen Welselski tritt alle drei bis vier Jahre vier Jahre in Deutschland auf, permanent in der Glotze, dann Streik, aber dann auch wieder in der Versenkung für Jahre. Wie oft ist denn dieser Vulkan in Island immer? Das nervt ja auch nur. Jo, kann's nicht fliegen? Ja, aber Island ist nicht so oft wie Welselski oder Carpenthal. Gott! Der Edna, der kommt alle zwei Jahre ums Eck. Kannst mal sehen, ganz andere Taktik. Ja, von der Außergewöhnlichkeit der Erscheinung ist das seltenste der Polsprung soll auch demnächst wieder soweit sein. Entwürft, wie meinen Sie meinen Sie das? -Dann wird der magnetische Nordpol zum Südpol. -Oha! -Und dann gibt das einen Kurzschluss oder was? -Das ist royal. Das ist alle hunderttausend Jubiläume. Dann kommt El Niño alle sieben, acht Jahre. Oha! Und dann kommt, mein ich schon, Weselsky. Früher wurde auch Frank Bzirski immer viel, ne? Ja, alle ein, zwei Jahre nicht besonders. Und diese Pocher-Trennung, die sind ja auch so wiederkehrend und nervig in meine Bild. Wochenlang denn dieses Rosenkriech-Gelaber, was kein Arsch wissen will? Dann ist das auch alles zu viel, weißt du? Bahnstreik, Pocher, Carbondale-Werbespots und es trifft dich wieder unerwartet. Doch wir haben auch das Rüstzeug, uns solchen wiederkehrenden Krisen wie Klaus Wieselski oder Oliver Pocher Trennung zu stellen. Ja, es beginnt mit der Akzeptanz. Ganz genau, und dann spielen wir die sieben Säulen der Resilienz durch. Oh, nein! Und kommen so auch durch diese neue Pocher-Phase und durch diesen erneuten Bahnstreik hindurch. By the way, Horst macht nur seinen Job, und den macht er sehr gut. Ah, man kennt sich? Natürlich. Ich dachte, er heißt Klaus Wieselski. Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das... Ja, das...iegen, ohne Fluchschorm und mit gutem Gewissen, aus Festland. Du hast ja kaum eine andere Chance. -Mh, ja? Was wollen Sie auf dem Festland, Frau Fass? -Leute gucken. -Ah.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"/Users/svenasmussen/Documents/Code/huggingFaceModels/whisper-large-v2-cv11-german\")\n",
    "model_sample_rate = processor.feature_extractor.sampling_rate\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"/Users/svenasmussen/Documents/Code/huggingFaceModels/whisper-large-v2-cv11-german\",\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Specify the path to your local audio file\n",
    "audio_file = \"AU-20231115-1359-0200.mp3\"\n",
    "\n",
    "prediction = pipe(audio_file, batch_size=1)\n",
    "\n",
    "print(prediction['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With manual setting of language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svenasmussen/Documents/Code/audioTranscribe/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, pipeline\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"/Users/svenasmussen/Documents/Code/huggingFaceModels/whisper-large-v2-cv11-german\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"/Users/svenasmussen/Documents/Code/huggingFaceModels/whisper-large-v2-cv11-german\", language=\"german\", task=\"transcribe\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/Users/svenasmussen/Documents/Code/huggingFaceModels/whisper-large-v2-cv11-german\")\n",
    "forced_decoder_ids = tokenizer.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=10,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be er Heimat, habe die Ehre! Grüß Sie ganz herzlich. Zehn Uhr ist es die Zeit, könnte man schon an ein Weißwurstfrühstück denken, ja, da haben Sie recht, das wäre jetzt genau das Richtige. Leider können wir Sie übers Radio nicht servieren, aber ein bisschen über die Weißwurst reden, das können wir natürlich schon. Denn wenn Sie in München fragen, wo gehe ich denn hin, wo kriege ich denn richtig gute Weißwurst, da würden viele Münchner sagen, ja, da musst zum Wallner in die Gaststätte Großmarkthalle in Sendling im Bauch von München. Und der Wirt, der Ludwig Wallner, der ist heute bei uns zu Gast. Schön, dass Sie sich Zeit genommen haben, da freuen wir uns. Grüß Gott miteinander, oder grüß Gott. Vielen Dank für die recht herzliche Einladung. Also wir haben jetzt hier bloß einen Kaffee stehen unter Wasser. Und und wie wär das jetzt die Uhrzeit? Sie fangen sehr früh an, haben Sie mir erzählt, kurz nach fünf ist es schon in Betrieb, der macht ja auch sieben auf. Hätten Sie da schon ein paar Eiswürstchen?\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Specify the path to your local audio file\n",
    "audio_file = \"231106_1005_Habe-die-Ehre_Ludwig-Wallner-von-der-Gaststaette-Grossmar.mp3\"\n",
    "\n",
    "# Load audio file\n",
    "audio = AudioSegment.from_mp3(audio_file)\n",
    "\n",
    "# Slice the first minute\n",
    "one_minute_audio = audio[:60000]  # pydub works in milliseconds\n",
    "\n",
    "# Save to a temporary file\n",
    "temp_file = \"temp.mp3\"\n",
    "one_minute_audio.export(temp_file, format=\"mp3\")\n",
    "\n",
    "# Use the temporary file for prediction\n",
    "prediction = asr_pipe(temp_file, batch_size=1, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids})\n",
    "\n",
    "print(prediction['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
