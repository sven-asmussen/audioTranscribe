{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\svena\\Documents\\GitHub\\audioTranscribe\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import os \n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"bofenghuang/whisper-large-v2-cv11-german\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"bofenghuang/whisper-large-v2-cv11-german\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# Assuming `model` is your trained model and `processor` is your processor\n",
    "save_directory = \"models/whisper-large-v2-cv11-german\"\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the processor/tokenizer\n",
    "processor.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With manual setting of language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, pipeline\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"models/whisper-large-v2-cv11-german\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"models/whisper-large-v2-cv11-german\", language=\"german\", task=\"transcribe\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"models/whisper-large-v2-cv11-german\")\n",
    "forced_decoder_ids = tokenizer.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=10,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file saved to temp.wav\n",
      "Be er Heimat, habe die Ehre! Ich begrüße Sie ganz herzlich. Zehn Uhr ist es die Zeit. Könnte man schon an ein Weißwurstfrühstück denken? Ja, da haben Sie recht. Das wäre jetzt genau das Richtige. Leider können wir Sie übers Radio nicht servieren, aber ein bisschen über die Weißwurst reden, das können wir natürlich schon. Denn wenn Sie in München fragen, wo gehe ich denn hin, wo kriege ich denn richtig gute Weißwurst, würden viele Münchner sagen, ja, da musst zum Wallner in die Gaststätte Großmarkthalle in Sendling im Bauch von München. Und der Wirt, der Ludwig Wallner, der ist heute bei uns zu Gast. Schön, dass Sie sich zeichnommen haben, da freuen wir uns. Grüß Gott miteinander, oder grüß Gott. Vielen Dank für die recht herzliche Einladung. Also wir haben jetzt hier bloß einen Kaffee stehen unter Wasser. und wie wär das jetzt die Uhrzeit? Sie fangen sehr früh an, haben Sie mir erzählt, kurz nach fünf ist es schon in Betrieb, der macht ja auch sieben auf. Hätten Sie da schon ein paar Eiswürstchen?\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "AudioSegment.converter = \"C:/ffmpeg/bin/ffmpeg.exe\"\n",
    "\n",
    "# Specify the path to your local audio file\n",
    "audio_file = \"demoFiles/InterviewMuenchen.mp3\"\n",
    "\n",
    "# Load audio file\n",
    "audio, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Slice the first minute\n",
    "one_minute_audio = audio[:sr*60]  # librosa works in samples, sr*60 gives the number of samples in one minute\n",
    "# one_minute_audio = audio[:sr*60]  # librosa works in samples, sr*60 gives the number of samples in one minute\n",
    "\n",
    "# Save to a temporary file\n",
    "temp_file = \"temp.wav\"\n",
    "sf.write(temp_file, one_minute_audio, sr)\n",
    "\n",
    "print(\"Audio file saved to\", temp_file)\n",
    "\n",
    "# Use the temporary file for prediction\n",
    "prediction = asr_pipe(temp_file, batch_size=1, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids})\n",
    "\n",
    "print(prediction['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
